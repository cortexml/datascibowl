#+DRAWERS: HIDDEN

* DONE Setup
  CLOSED: [2014-12-24 Wed 13:45]
  :LOGBOOK:  
  - State "DONE"       from ""           [2014-12-24 Wed 13:45]
  :END:      
  - create a data directory
  - create a file and put in it the download links
#+begin_src sh :results verbatim raw replace   
  projdir="$HOME/projects/2014-12-20_datascibowl/"
  if [ -d "$projdir" ]
  then
    echo "$projdir exists, entering it"
    cd "$projdir"
  else
    echo "making $projdir and entering it"
    mkdir "$projdir" && cd "$_"
  fi
  if [-d "data"]
  then
    echo "data dir exists, entering it"
    cd "data"
  else
    echo "creating data directory and download file list"  
    mkdir "data" && cd "data"
    data_download.txt
    echo 'http://www.kaggle.com/c/datasciencebowl/download/train.zip' > data_download.txt
    echo 'http://www.kaggle.com/c/datasciencebowl/download/test.zip' >> data_download.txt
  fi
#+end_src
#+RESULTS:
/home/joth/projects/2014-12-20_datascibowl/ exists, entering it
creating data directory and download file list


* DONE Get data
  CLOSED: [2014-12-24 Wed 13:45]
  :LOGBOOK:  
  - State "DONE"       from ""           [2014-12-24 Wed 13:45]
  :END:      
#+begin_src sh silent
  # currently doesn't work, need to export browser cookies
  # and use wget option --load-cookies
  # http://askubuntu.com/questions/161778/how-do-i-use-wget-curl-to-download-from-a-site-i-am-logged-into
  # only if files in data_download dont exist
  cd "$HOME/projects/2014-12-20_datascibowl/data"
  wget -nv -i data_download.txt
#+end_src
#+RESULTS:

#+begin_src sh silent
  # again if they dont already exist, that's -n
  cd ~/projects/2014-12-20_datascibowl/data
  unzip -n train
  unzip -n test
#+end_src


* DONE Ipython
  CLOSED: [2014-12-20 Sat 22:03]
  :LOGBOOK:  
  - State "DONE"       from "TODO"       [2014-12-20 Sat 22:03]
  :END:      
  - installed in arch, had a few dependencies, some from
    AUR I think, though that may have been for skimage
  ~ipython2 notebook~ 
  - opens ipython in web browser
  - from Help can see keyboard shortcuts, and links to
    libary documentation


* TODO Python/Dev Stuff [8/13]
** DONE Setup packages
   CLOSED: [2014-12-24 Wed 13:45]
   :LOGBOOK:  
   - State "DONE"       from ""           [2014-12-24 Wed 13:45]
   :END:      
   - python environment in [[http://antarch.calepin.co/setting-up-a-python-environment-in-arch-linux-basics.html][arch]]
   - it is suggested to manage packages with the distro
     package manager [[http://antarch.calepin.co/setting-up-a-python-environment-in-arch-linux-basics.html][see]]
   - from the [[http://nbviewer.ipython.org/github/udibr/datasciencebowl/blob/master/141215-tutorial.ipynb][tutorial]] it looks like we need
   - 2.7
     - extra
       - [X] sklearn scikit-learn
       - [X] matplotlib
       - [X] pylab
       - [X] numpy
       - [X] pandas
       - [X] scipy
       - [X] pillow
       - there were some more dependencies for iptyhon, and
         one more for a function called from the tutorials
       - imread was one of them
       - 
     - defaults
       - glob
       - os
       - warnings
       - pylab in venv
     - needed gcc-fortran for install scipy in venv

** DONE virtual env Research
   CLOSED: [2014-12-23 Tue 01:24] SCHEDULED: <2014-12-22 Mon>
   :LOGBOOK:  
   - State "DONE"       from "TODO"       [2014-12-23 Tue 01:24]
   :END:      
   - 
** DONE Setting up for python development on ArchLinux
    CLOSED: [2014-12-24 Wed 13:44]
    :LOGBOOK:  
    - State "DONE"       from "NEXT"       [2014-12-24 Wed 13:44]
    - State "NEXT"       from ""           [2014-12-24 Wed 12:47]
    :END:      
    - References
      - [[http://docs.python-guide.org/en/latest/dev/virtualenvs/][pydocs]]
      - [[https://wiki.archlinux.org/index.php/Python_VirtualEnv][archwiki]]

First we're going to isntall pip and virtual env, we dont
need pip as venvs install pip into the venv when created,
but worth getting it for shell completion.
#+begin_src sh :noeval
sudo pacman -S python3-pip
sudo pacman -S python3-virtualenv
# sudo pacman -S python2-pip
# sudo pacman -S python2-virtualenv
#+end_src

Make pip play nice with zsh.
#+begin_src sh
pip completion --zsh >> ~/.zprofile
#+end_src

venvwrapper is a nubmer of shell scripts that simplify
working with and tracking venvs. I regret it fixes the
virtual env base path, which affects how I organize
projects. Although venvs can be linked to [[https://virtualenvwrapper.readthedocs.org/en/latest/projects.html][project directory]]
using mkproject, which satisfies my purposes.
#+begin_src python :noeval
#sudo pip install virtualenvwrapper
sudo pacman -S python-virtualenvwrapper # can create python2 venvs
echo "source /usr/local/bin/virtualenvwrapper.sh" >> ~/.zprofile
export WORKON_HOME=~/venvs
export PROJECT_HOME=~/projects
mkdir -p $WORKON_HOME
echo "export WORKON_HOME=$WORKON_HOME" >> ~/.zprofile
echo "export PROJECT_HOME=$PROJECT_HOME" >> ~/.zprofile
#+end_src

Head over to the project directory and setup a venv there,
then activate it.
#+begin_src sh :noeval
#virtualenv -p /usr/bin/python2.7 workspace_datascibowl
# mkvirtualenv -p /usr/bin/python2.7 2014-12-23_datascibowl

# -f makes the venv even if proj exists
mkproject -f -p /usr/bin/python2.7 2014-12-24_testdatascibowl

# check out your venvs
ls $WORKON_HOME
#+end_src

#+begin_src sh
pip install git://github.com/yajiemiao/pdnn
# check out packages in venv
lssitepackages
#+end_src

~deactivate~ deactivates the current venv. And can also
navigate to the active venv with ~cdvirtualenv~.

** DONE Settung up python dev on ArchLinux v2
   CLOSED: [2015-01-06 Tue 22:04]
   :LOGBOOK:  
   - State "DONE"       from "NEXT"       [2015-01-06 Tue 22:04]
   - State "NEXT"       from "TODO"       [2015-01-06 Tue 08:34]
   :END:      
   - missed the point, mkproject not working, it's putting
     the venv in the project dir instead of the $WORKON_HOME
   - retested - mkproject works
   - for future reference, this is how to do this if there's
     alredy a project dir
   - so how to do this? just make the venv and then
     associate it with a project dir?
#+begin_src sh :noeval
mkvirtualenv -p /usr/bin/python2.7 2014-12-20_datascibowl
setvirtualenvproject  ~/projects/2014-12-20_datascibowl
#or 
mkvirtualenv -p /usr/bin/python2.7 -a ~/projects/2014-12-20_datascibowl  2014-12-20_datascibowl 
cdvirtualenv # takes to venv dir
cdproject # takes to project dir of active venv
#+end_src

** DONE setting up venv for distribution
   CLOSED: [2015-01-06 Tue 22:44]
   :LOGBOOK:  
   - State "DONE"       from "NEXT"       [2015-01-06 Tue 22:44]
   - State "NEXT"       from ""           [2015-01-06 Tue 22:43]
   :END:      
   - make requirements list

#+begin_src sh :noeval
pip freeze > ~/venvs/2014-12-20_datascibowl/requirements.txt
#+end_src

#+begin_src sh
cat $WORKON_HOME/2014-12-20_datascibowl/requirements.txt
#+end_src

#+RESULTS:
| Jinja2==2.7.3                         |
| MarkupSafe==0.23                      |
| Pillow==2.7.0                         |
| backports.ssl-match-hostname==3.4.0.2 |
| certifi==14.05.14                     |
| ipython==2.3.1                        |
| matplotlib==1.4.2                     |
| mock==1.0.1                           |
| nose==1.3.4                           |
| numpy==1.9.1                          |
| pandas==0.15.2                        |
| pyparsing==2.0.3                      |
| python-dateutil==2.4.0                |
| pytz==2014.10                         |
| pyzmq==14.4.1                         |
| scikit-image==0.10.1                  |
| scikit-learn==0.15.2                  |
| scipy==0.14.1                         |
| six==1.9.0                            |
| tornado==4.0.2                        |
| wsgiref==0.1.2                        |

   - pip install -r 
   - ipython
   - pyzmq
   - jinja2
   - tornado

** DONE [[https://docs.python.org/2/library/pickle.html][Pickle]] for saving python binary objects 
   CLOSED: [2015-01-09 Fri 21:18]
   :LOGBOOK:  
   - State "DONE"       from "NEXT"       [2015-01-09 Fri 21:18]
   - State "NEXT"       from "TODO"       [2015-01-06 Tue 13:49]
   - State "NEXT"       from "TODO"       [2015-01-06 Tue 13:37]
   :END:      
*** why pickle?
    Pickle lets you make persistent python objects.
    Intermediate results, such as feature matrices and model
    objects, may be costly to compute. It is often useful
    then to save them. This may also aid reproduction.
    Pickle allows for this by converting python objects to
    byte streams.

*** what can pickle do?
    Pickle pickles (serializes) python objects into byte
    streams. It also unpicks (or unserializes) them, i.e.
    the byte stream is loaded back into a python object. The
    byte streams can be saved to persistent storage, in a
    database, or transmitted over a network.

    If you know the nitty gritty send me a link at the
    contact page, twitter, or leave a comment.

*** What can't be pickled?
    Generally speaking, classes, functions, and methods. 

*** how to use it?
    For pickling (unpickling), create a pickle object and
    call its dump (load) method with a file connection.

#+begin_src python :results none  
import pickle

wrc_champions = {
  "Sebastien Loeb": [i+2004 for i in range(9)],
  "Juha Kankkunen": [1986,1987,1991,1993]
}

with open("wrc_champions.p", "w") as f:
  pickle.dump(wrc_champions, f)

#+end_src



#+begin_src python :results verbatim org replace output 
import pickle
import pprint
import os
with open("wrc_champions.p") as f:
  wrc_champions = pickle.load(f)

pprint.pprint(wrc_champions)
os.remove("wrc_champions.p")
#+end_src

#+RESULTS:
#+BEGIN_SRC org
{'Juha Kankkunen': [1986, 1987, 1991, 1993],
 'Sebastien Loeb': [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]}
#+END_SRC

*** cPickle
    The cPickle module has Pickler and Unpickler methods,
    which are up to 1000 times faster than the Pickle
    counterparts. I wonder what the limitations are to that.
    Although, cPickle ones can't be [[https://docs.python.org/2/library/pickle.html#pickle-sub][subclassed]], they suit
    most common purposes. There are [[https://wiki.python.org/moin/UsingPickle][other reasons]] to stick
    with pickle, though.

*** Caveats (Carrots?)
    Dont eat pickles from untrusted sources, arbitrary
    python code can be executed in unpickling! Found [[https://wiki.python.org/moin/UsingPickle][here]],
    along with more pertinents.

** DONE Sets and lists [[https://docs.python.org/2/tutorial/datastructures.html][builting data structures]]
    CLOSED: [2014-12-24 Wed 20:50]
    :LOGBOOK:  
    - State "DONE"       from ""           [2014-12-24 Wed 20:50]
    :END:      
Sets are hashed iterable unorder lists of distinct items.
#+begin_src python :results scalar org replace output  
a = ["why'd", "you", "crisp", "greedo?"]
set_a = set(a)
type(a)
type(set_a)

print "List a:"
for item in a:
  print item

print "Set a:"
for item in set_a:
  print item

a.append(", han")
print a

set_b = (["why'd", "you", "crisp", "greedo?"])
print set_a.difference(set_b)
#+end_src

#+RESULTS:
#+BEGIN_SRC org
List a:
why'd
you
crisp
greedo?
Set a:
you
crisp
greedo?
why'd
["why'd", 'you', 'crisp', 'greedo?', ', han']
set(['you', 'crisp', 'greedo?', "why'd"])
#+END_SRC
** DONE list comprehensions
   CLOSED: [2015-01-09 Fri 21:21]
   :LOGBOOK:  
   - State "DONE"       from "TODO"       [2015-01-09 Fri 21:21]
   :END:      
   - did an online python tutorial
   - dont even remember why!
** NEXT [[https://wiki.python.org/moin/Generators][generators]]
   :LOGBOOK:  
   - State "NEXT"       from "TODO"       [2015-01-09 Fri 21:21]
   :END:      
   
** TODO learn key python packages [2/6]
   - for this i changed the python command variable in emacs
     to use python2
   - also must use :results output tag instead of :results
     value, as the latter requires you return object
*** TODO [[http://docs.scipy.org/doc/numpy/reference/][Numpy]] [2/3]
    :LOGBOOK:  
    - State "DONE"       from ""           [2014-12-21 Sun 21:14]
    :END:      
**** DONE creating arrays
     CLOSED: [2015-01-06 Tue 22:47]
     :LOGBOOK:  
     - State "DONE"       from ""           [2015-01-06 Tue 22:47]
     :END:      

#+begin_src python :results  code replace output 
import numpy as np
x = np.array([[1, 2, 3], [4, 5, 6]], np.int32)
print type(x)
print x
print x.shape
print x[0,1]
print x[1,2]
print x[:,1]
print x[:1,]
#+end_src

#+RESULTS:
#+BEGIN_SRC python
<type 'numpy.ndarray'>
[[1 2 3]
 [4 5 6]]
(2, 3)
2
6
[2 5]
[[1 2 3]]
#+END_SRC
**** DONE [[http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#arrays-indexing][slicing]]
     CLOSED: [2015-01-06 Tue 22:47]
     :LOGBOOK:  
     - State "DONE"       from ""           [2015-01-06 Tue 22:47]
     :END:      
     - is powerful, and slices are pointers, i.e. updating
       their contents updates the parent object
     - slice with array[i:j:k] where i is start index, j is
       stop, and k int > 0 is step
     - seems to be indexed as so
       |---+---+---|
       | 0 | 2 | 4 |
       | 1 | 3 | 5 |
       |---+---+---|

#+begin_src python :results  code replace output 
import numpy as np
x = np.array([[1, 2, 3], [4, 5, 6]], np.int32)
print x[1:5:2]
print x[:,1]
print x[:1,]

#+end_src

#+RESULTS:
#+BEGIN_SRC python
[[4 5 6]]
[2 5]
[[1 2 3]]
#+END_SRC

**** NEXT [[http://docs.scipy.org/doc/numpy/reference/arrays.nditer.html][iteration]]
     :LOGBOOK:  
     - State "NEXT"       from ""           [2015-01-06 Tue 22:47]
     :END:      
     - 

**** [[http://docs.scipy.org/doc/numpy/reference/routines.linalg.html][linalg]]
     - 
*** DONE [[http://docs.scipy.org/doc/numpy/reference/][Scipy]]
    CLOSED: [2015-01-04 Sun 13:18]
    :LOGBOOK:  
    - State "DONE"       from "TODO"       [2015-01-04 Sun 13:18]
    :END:      
    - mathematical functions built on numpy
    - read about it.
      - stats functions,
      - spatial data method such as delaunay triangulation,
      - singal proc
      - integration, ode
    - there is a [[http://docs.scipy.org/doc/scipy/reference/tutorial/index.html][tutorial]]
*** NEXT [[http://docs.scipy.org/doc/scipy/reference/tutorial/ndimage.html][multidimensional image proc]]
    :LOGBOOK:  
    - State "NEXT"       from "TODO"       [2015-01-09 Fri 21:22]
    - State "NEXT"       from "TODO"       [2015-01-09 Fri 21:22]
    - State "NEXT"       from "TODO"       [2015-01-09 Fri 21:22]
    - State "NEXT"       from "TODO"       [2015-01-09 Fri 21:21]
    :END:      
    - i dont think
    - relevant for snodrofo
*** NEXT Pandas
    :LOGBOOK:  
    - State "NEXT"       from "TODO"       [2015-01-09 Fri 21:22]
    :END:      
    - 
*** DONE [[http://scikit-learn.org/stable/user_guide.html#user-guide][sci kit learn (user guide)]]
    CLOSED: [2015-01-06 Tue 08:36]
    :LOGBOOK:  
    - State "DONE"       from "TODO"       [2015-01-06 Tue 08:36]
    :END:      
    - the [[http://scikit-learn.org/stable/modules/linear_model.html][help section on regression]] is easy as i already
      understand that, understanding the python part is
      straight forward
    - and the [[http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html][algorithm map]] is very useful
*** NEXT sci kit image
    :LOGBOOK:  
    - State "NEXT"       from "TODO"       [2015-01-09 Fri 21:22]
    :END:      
    - 
** CANCELLED Hu central moment expirimentation [0/2]
   :LOGBOOK:  
   - State "CANCELLED"  from "TODO"       [2015-02-05 Thu 12:50] \\
     - dnn
   :END:      
*** TODO Features [1/2]
   - [X] try using moments 2-6, I htink 1-7 are not as
     meaninful
     - no better
   - [ ] try permutation of the central moments combinations

*** NEXT Tests [0/1]
    :LOGBOOK:  
    - State "NEXT"       from ""           [2015-01-06 Tue 08:35]
    :END:      
   - [ ] try manually doing the central moments on a trivial
     image to test the reflect, translation, rotation
     invariance

** NEXT ML procedure
   :LOGBOOK:  
   - State "NEXT"       from ""           [2015-01-09 Fri 22:52]
   :END:      
   - read data
   - save pickled raw data? maybe with cpickle
   - calculate features and create descriptive stats on them
   - save feature matrices
   - fit models
** model infrastructure
   - how can we optimize the iteration
   - save intermediate models with pickle
   - bert is doing this
** Write a class for this? work with bert
   - ugh, i like the functional/R paradigm of methods belong
     to functions! this hurts my brain
   - inside venv we shoudl have all we need to work on it,
     including the data and libraries
   - class is datascibowl
   - attributes
     - training classes
       - nested dict with names of classes/subclasses?
   - methods
     - scale image
     - calculate feature
     - preprocess
     - 

* TODO ML Stuff [1/11]
** TODO [[http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial][ufldl tutorial]] for deep learning
   - 
** DONE Check the kaggle handwriting competition example
   CLOSED: [2015-01-04 Sun 12:39] SCHEDULED: <2014-12-26 Fri>
   :LOGBOOK:  
   - State "DONE"       from "NEXT"       [2015-01-04 Sun 12:39]
   - State "NEXT"       from "TODO"       [2014-12-25 Thu 16:25]
   :END:      
   - bert thinks may be some inspiration here
   - I am not seeing much other than tutorial/info on RF
     classifier
   - RF decision trees

** CANCELLED make a uniform probability prediction
   SCHEDULED: [2014-12-26 Fri]
   :LOGBOOK:  
   - State "CANCELLED"  from "NEXT"       [2015-01-04 Sun 12:39] \\
     trivial
   - Rescheduled from "2014-12-22 Mon" on [2014-12-25 Thu 16:29]
   - State "NEXT"       from "TODO"       [2014-12-21 Sun 21:13]
   :END:      
   The idea here is just to learn a bit more about the data
   manipulations, processing, etc in python.
   Cancelled - trivial.
*** data
    - submission of form
    | imagefilename | class1 prob | class2 prob | ... | classn prob |
    | 1.jpg         | 1/n         | 1/n         | 1/n | 1/n         | 
** NEXT Understand the error/performance measure better
   - 
** TODO Preprocessing
   - we have to identify our features, at the same time as
     not introducing some censorship of the dater
   - tutorial uses
     - thresholding on mean to reduce noise
     - dilate to connect neighboring pixels
     - segmention of connection regions - calculate their
       labels (e.g. region 1,2,3)
     - apply the labels to original images
*** NEXT check prportion of obs with maxRegion [/]
    :LOGBOOK:  
    - State "NEXT"       from "TODO"       [2015-01-09 Fri 22:52]
    :END:      
    - when no maxregion, axis region ratio is 0, same with
      hu centres for now
    - [ ] get the proportion which are 0
** TODO feature brainstorming
   - Recall, manual feature selection is a difficult
     endeavour, rather search the feature space and have a
     generalization to create features, e.g. hu moments and
     thier interactions
   - aspect ratio
   - eccentricity
   - something about the protrusions
   - how about degree of transparency/variance of pixel
     values in feature
   - most of these i reckon are captured by the hu moments
*** DONE hu central moments Research
    CLOSED: [2015-01-04 Sun 21:14] SCHEDULED: <2014-12-25 Thu>
    :LOGBOOK:  
    - State "DONE"       from ""           [2015-01-04 Sun 21:14]
    :END:      
    - and see [[Hu%20central%20moment%20expirimentation][Hu central moment expirimentation]] for tests to
      try
    - see the wiki [[http://en.wikipedia.org/wiki/Image_moment][here]]
    - image moments are certain averages of image pixel
      intensities, and functions of such first moments
    - this is defined for discrete greyscale data as

\begin{equation}
  M_{ij} = \sum_x \sum_y x^i y^j I(x,y)
\end{equation}

    - they usually are formulated to have an attractive
      property, e.g. the hu central moments are rotation,
      translation

:HIDDEN:
\begin{equation}
   I_1 = \eta_{20} + \eta_{02}
\end{equation}
\begin{equation}
   I_2 = (\eta_{20} - \eta_{02})^2 + 4\eta_{11}^2
\end{equation}
\begin{equatio}
   I_3 = (\eta_{30} - 3\eta_{12})^2 + (3\eta_{21} - \eta_{03})^2
\end{equation}
\begin{equatio}
   I_4 = (\eta_{30} + \eta_{12})^2 + (\eta_{21} + \eta_{03})^2
\end{equation}
\begin{equatio}
   I_5 = (\eta_{30} - 3\eta_{12}) (\eta_{30} + \eta_{12})[ (\eta_{30} + \eta_{12})^2 - 3 (\eta_{21} + \eta_{03})^2] + (3 \eta_{21} - \eta_{03}) (\eta_{21} + \eta_{03})[ 3(\eta_{30} + \eta_{12})^2 -  (\eta_{21} + \eta_{03})^2]
\end{equation}
\begin{equatio}
   I_6 =  (\eta_{20} - \eta_{02})[(\eta_{30} + \eta_{12})^2 - (\eta_{21} + \eta_{03})^2] + 4\eta_{11}(\eta_{30} + \eta_{12})(\eta_{21} + \eta_{03})
\end{equation}
\begin{equatio}
   I_7 = (3 \eta_{21} - \eta_{03})(\eta_{30} + \eta_{12})[(\eta_{30} + \eta_{12})^2 - 3(\eta_{21} + \eta_{03})^2] - (\eta_{30} - 3\eta_{12})(\eta_{21} + \eta_{03})[3(\eta_{30} + \eta_{12})^2 - (\eta_{21} + \eta_{03})^2].
\end{equation}
:END:

   - how does this transfer to image/pattern recognition?
     Any image can be represented as a density function in
     x,y, which can further be represented by these moments.
     these moments should be invariant with respect to the
     images position in the visual field and pattern size.
   - so *why* does the python method for getting these
     moments take a region centroid? maybe it's to mask out
     parts of the image, but in this case, where the rest of
     the image is blank, I don't see it mattering
   - so, of these moments, 7 may be bad as it represents
     reflected images, and 1, the centroid, I don't think is
     pertinent
   - what of the others? the original [[http://www.sci.utah.edu/~gerig/CS7960-S2010/handouts/Hu.pdf][ieee transactions]]
     document
   - regardless, my question is, if i add crappy features,
     why is the classifier using them and scoring so
     poorly?!

*** NEXT image feature extraction
    :LOGBOOK:  
    - State "NEXT"       from "TODO"       [2015-01-06 Tue 13:40]
    :END:      
    - [[http://scikit-learn.org/stable/modules/feature_extraction.html#image-feature-extraction][link to image feat extr section]] on scikitlearn feat
      extr page
** TODO different models than RF
   - 
** TODO model fitting
   - remember, the feature array is just a matrix where the
     features are some function/measure of the image
   | class | feature 1 | feature 2 | ... | feature m |
   |       |           |           |     |           |
*** TODO RF [0/1]
    - [ ] why is the classifier using features that give it
      lower performance?! i.e. eccentricity ratio is good,
      naive hu is good, but region centred HU with 0s or
      naive hu for None largest regions sucks!
** TODO *nested model* 
   - There are many families of plankton, and there is some
     similarity with sub classes, can we capture this
   - Some of the subclasses are even the same as others, but
     from different views
     - e.g. hydromedusae partial dark vs ShapeA
     - but we still have to predict each class as given in
       the folders
** TODO Different number of samples per class
   - shall we predict smaller samples classes with less
     confidence?

** TODO Java deep learning library
   - http://deeplearning4j.org/quickstart.html
   - from bert


* TODO Blog topics
** DONE getting the data in
   CLOSED: [2015-02-05 Thu 12:59]
   :LOGBOOK:  
   - State "DONE"       from ""           [2015-02-05 Thu 12:59]
   :END:      
Duplicate the minst data setup.

#+begin_src sh :noeval
# clone my fork of pylearn git clone git://
git branch -c datascibowl
cdproject
cp pylearn2/datasets/mnist.py pylearn2/datasets/datascibowl.py
cp pylearn2/utils/mnist-ubyte.py pylearn2/datascibowl_reader/datascibowl.py
#+end_src

The main thing to do with data for pylearn2 is to subclass
the module's data objects. In this case, we are using
denseDesignMatrix. Let's look at the relevant info from the
docstring:

#+begin_src python :noeval
class DenseDesignMatrix(Dataset):

    """

    A class for representing datasets that can be stored as a \
    dense design matrix (and optionally, associated targets).

    Parameters
    ----------
    topo_view : ndarray, optional
        Should be supplied if X is not.  An array whose first \
        dimension is of length number examples. The remaining \
        dimensions are examples with topological significance, \
        e.g. for images the remaining axes are rows, columns, \
        and channels.
    y : ndarray, optional

        Targets for each example (e.g., class ids, values to be predicted
        in a regression task).

        - 2D ndarray, data type optional:
            This is the most common format and can be used for a variety
            of problem types. Each row of the matrix becomes the target
            for a different example. Specific models / costs can interpret
            this target vector differently. For example, the `Linear`
            output layer for the `MLP` class expects the target for each
            example to be a vector of real-valued regression targets. (It
            can be a vector of size one if you only have one regression
            target). The `Softmax` output layer of the `MLP` class expects
            the target to be a vector of N elements, where N is the number
            of classes, and expects all but one of the elements to 0. One
            element should have value 1., and the index of this element
            identifies the target class.
    axes: tuple, optional
        The axes ordering of the provided topo_view. Must be some permutation
        of ('b', 0, 1, 'c') where 'b' indicates the axis indexing examples,
        0 and 1 indicate the row/cols dimensions and 'c' indicates the axis
        indexing color channels.
    y_labels : int, optional
        If y contains labels then y_labels must be passed to indicate the
        total number of possible labels e.g. 10 for the MNIST dataset
        where the targets are numbers. This will make the set use
        IndexSpace.
    """
#+end_src

So, to subclass denseDesignMatrix for the data in question,
the following are needed:

- ~topo_view~
  - an ndarray with the following format
  (examples, parameters_indicating_topological_significance)
  - where parameters_indicating_toplogical_significance
    describe the structure of the data. Rows, columns, and
    channels is given as an example. It is easy to imagine
    setting up data in 3d space and temperature x-coords,y
    coords,z coords, temperature, for example
  - this is also what will actually hold the data in the
    examples space
  - e.g. we have (n, x, y) where n are ixj matrices of pixel
    values and x,y are simply i,j respectively
  - reshaping this to (n,x,y,1) signifies that n is
    comprised of 1 value, whereas n,x,y,3 would let us use,
    e.g., RGB

- ~y~
  - targets, each row corresponds to the targets for each
    example
  - for this example, this should be an ndarray with the
    number of elements that we have classes, and all but one
    element having value 0 (i.e. each example belongs to one
    class)
  - for example, imagine we have 3 mutually exclusive
    classes to predict. ~y~ may look like
| 0 | 0 | 1 |
| 1 | 0 | 0 |
| 1 | 0 | 0 |
| 0 | 0 | 1 |
| 0 | 0 | 1 |

- ~axes~
  - permutations of (b,0,1,c) where
  - b indicates examples
  - 0,1 indicates rows/columns(i dont understand this
    convention, perhaps 0th and 1st dimension)
  - and c indicates the colors

- ~y_labels~
  - an integer containing the total number of possible
    labels, if y contains labels

** DONE DataSciBowl options [2/2]
   CLOSED: [2015-02-06 Fri 15:45]
   :LOGBOOK:  
   - State "DONE"       from ""           [2015-02-06 Fri 15:45]
   :END:      
   - [X] train and test
     - actually, test folder in data is for prediction only
     - do not use 'test'
   - [X] total number of images for start - stop
   - 
** DONE Test DataSciBowl class
   CLOSED: [2015-02-06 Fri 16:03]
   :LOGBOOK:  
   - State "DONE"       from ""           [2015-02-06 Fri 16:03]
   :END:      
   I could use the datascibowl_reader easily enough, but
   couldn't manage to test the dataset class itself.

   I tried just copying the class def to ipython and
   instantiating, but it doesnt work. Which is so odd.
   Because creating a DenseDesignMatrix works just fine.
   Wtf. For now, I'm going to move on and see about setting
   up a model.

   It was because of a bad path. Working fine now.

** DONE Model setup with pylearn2
   CLOSED: [2015-02-06 Fri 16:18]
   :LOGBOOK:  
   - State "DONE"       from ""           [2015-02-06 Fri 16:18]
   :END:      
   In the pylearn directory ~scripts/~ there are great
   tutorials. 

   With a dataset class, the next step is training a model.
   The convenient way to do this is with yaml docs. With
   some adaptation from existing examples, mine should look
   like:

#+begin_src sh :results verbatim code replace output 
cat sr_train.yaml
#+end_src
#+RESULTS:
#+BEGIN_SRC sh
!obj:pylearn2.train.Train {
    "dataset": &train !obj:pylearn2.datasets.datascibowl.DataSciBowl {
        which_set: 'train',
        start: 0,
        stop: 25000,
	},
    "model": !obj:pylearn2.models.softmax_regression.SoftmaxRegression {
        n_classes: 121,
        irange: 0.,
        nvis: 1024,
    },
    "algorithm": !obj:pylearn2.training_algorithms.bgd.BGD {
        batch_size: 5000,
        line_search_mode: 'exhaustive',
        conjugate: 1,
        monitoring_dataset:
            {
                'train' : *train,
                'valid' : !obj:pylearn2.datasets.datascibowl.DataSciBowl {
                              which_set: 'train',
                              start: 25000,
                              stop:  30336,
                          }
            },
        termination_criterion: !obj:pylearn2.termination_criteria.MonitorBased {
            channel_name: "valid_y_misclass"
        }
    },
    "extensions": [
        !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {
             channel_name: 'valid_y_misclass',
             save_path: "softmax_regression_best.pkl"
        },
    ],
    save_path: "softmax_regression.pkl",
    save_freq: 1
}
#+END_SRC

   There's quite a bit going on there; the dataset, model,
   and fitting algorithm are specified along with some
   information for test/validation data. Feel free to leave
   a comment if the syntax is confusing and I will try to
   clarify.

   I want to highlight that several datasets can be used to
   monitor the fitting. Their names are arbitrary. I am
   using 'train' and 'valid', where valid is just a subset
   of the classified data reserved for testing.

monitoring_dataset : Dataset or dictionary, optional

    If not specified, no monitoring is used. If specified to
    be a Dataset, monitor on that Dataset. If specified to
    be dictionary, the keys should be string names of
    datasets, and the values should be Datasets. All
    monitoring channels will be computed for all monitoring
    Datasets and will have the dataset name and an
    underscore prepended to them

** NEXT Fit a model
   :LOGBOOK:  
   - State "NEXT"       from ""           [2015-02-06 Fri 16:37]
   :END:      
See softmax_regression.ipynb.

On first attempt I get an error.

~ValueError: Input dimension mis-match~

[[https://groups.google.com/forum/?utm_source=digest&utm_medium=email#!searchin/pylearn-users/input$20dimension$20mis-match/pylearn-users/vpSSiko0I4o/Gjuhl62TUyIJ][the google groups has seen such a thing]] before. A good next
step is to play with the batch size, and to add theano flags
~exception_verbosity=high~ and ~optimizer=fast_compile~ or
~none~ to get a trace of which pylearn call is irritating
theano.

I'm not sure where to add these.

[[http://deeplearning.net/software/theano/library/config.html][The docs]] note that theano looks at ~$HOME/.theanorc~

Write those options to that file and try again.

#+begin_src sh :noeval
echo "[global]" > /home/joth/.theanorc
echo "exception_verbosity = high" >> /home/joth/.theanorc
echo "optimizer = fast_compile" >> /home/joth/.theanorc
#+end_src

Still no success. But now with more debug info.

I tried a few modifications to the sr_train.yaml to rule out
plausible simple mistakes with no success.

Someone on the forums recommended me to try something.
[[https://groups.google.com/forum/?utm_source=digest&utm_medium=email#!topic/pylearn-users/JaOAPWlIi8w][here's]] the tread.

"One way to figure out what is happening would be to put a
breakpoint in monitor.py a bit before line 255, and check
what comes out of the data iterator (X), in particular the
shape of all elements in the tuple, and if they seem to
correspond to inputs or targets of examples in the dataset."

Let's look at the call method of monitor.py

#+begin_src python :noeval
    def __call__(self):
        """
        Runs the model on the monitoring dataset in order to add one
        data point to each of the channels.
        """

        # If the channels have changed at all, we need to recompile the theano
        # functions used to compute them
        if self._dirty:
            self.redo_theano()

        datasets = self._datasets

        # Set all channels' val_shared to 0
        self.begin_record_entry()
        for d, i, b, n, a, sd, ne in safe_izip(datasets,
                                               self._iteration_mode,
                                               self._batch_size,
                                               self._num_batches,
                                               self.accum,
                                               self._rng_seed,
                                               self.num_examples):
            if isinstance(d, six.string_types):
                d = yaml_parse.load(d)
                raise NotImplementedError()

            # need to put d back into self._datasets
            myiterator = d.iterator(mode=i,
                                    batch_size=b,
                                    num_batches=n,
                                    data_specs=self._flat_data_specs,
                                    return_tuple=True,
                                    rng=sd)

            # If self._flat_data_specs is empty, no channel needs data,
            # so we do not need to call the iterator in order to average
            # the monitored values across different batches, we only
            # have to call them once.
            if len(self._flat_data_specs[1]) == 0:
                X = ()
                self.run_prereqs(X, d)
                a(*X)

            else:
                actual_ne = 0
                for X in myiterator:
                    # X is a flat (not nested) tuple
                    self.run_prereqs(X, d)
                    a(*X)
                    actual_ne += self._flat_data_specs[0].np_batch_size(X)
                # end for X
                if actual_ne != ne:
                    raise RuntimeError("At compile time, your iterator said "
                                       "it had %d examples total, but at "
                                       "runtime it gave us %d." %
                                       (ne, actual_ne))
        # end for d

        log.info("Monitoring step:")
        log.info("\tEpochs seen: %d" % self._epochs_seen)
        log.info("\tBatches seen: %d" % self._num_batches_seen)
        log.info("\tExamples seen: %d" % self._examples_seen)
        t = time.time() - self.t0
        for channel_name in sorted(self.channels.keys(),
                                   key=number_aware_alphabetical_key):
            channel = self.channels[channel_name]
            channel.time_record.append(t)
            channel.batch_record.append(self._num_batches_seen)
            channel.example_record.append(self._examples_seen)
            channel.epoch_record.append(self._epochs_seen)
            val = channel.val_shared.get_value()
            channel.val_record.append(val)
            # TODO: use logging infrastructure so that user can configure
            # formatting
            if abs(val) < 1e4:
                val_str = str(val)
            else:
                val_str = '%.3e' % val

            log.info("\t%s: %s" % (channel_name, val_str))
#+end_src

For each dataset an iterator is set up. My work is breaking
in the comparison of the first two items the iterator
returns. It will also be useful then to look at the __next__
method for the default densedesignmatrix class and compare
it to the mnist one.

In python,

#+begin_src python :noeval
import pdb
pdb.set_trace() # at break location, on/as line 256
#+end_src

Debug learning:
:HIDDEN:
Some useful pdb commands

~(p)rint~ print a variable

~(s)tep~ to step forward one statement, whereas ~(n)ext~
will step forward a function call. If the line is not a
function call, ~s~ and ~n~ are equivalent.

~r~ continue until routine; lets you continue until the next
return, great way to escape functions if you reckon the
bug to not be present

~(q)uit~ 

~commands~ to enter literal commands, enter *end* to stop,
and n to step through the commands

And an excellent tutorial is [[https://pythonconquerstheuniverse.wordpress.com/2009/09/10/debugging-in-python/][here]]
:END:

That aside behind us, I got into the stack to check out what
the iterator is returning. ~X[0]~ are examples and ~X[1]~
should be log probabilities for each class.

#+begin_src python :noeval
(Pdb) X[0].shape
(5000, 1024)
(Pdb) X[1].shape
(1, 121)
#+end_src

~X[1]~ actually all have value 1.

Also, in ~X[0]~, the data may be degenerate. For most of the
examples, the max value is 0. This is confirmed in the raw
data.

#+begin_src python
image_mean = [np.reshape(i, (1, 1024)).mean() for i in test.X]
d = {x:image_mean.count(x) for x in image_mean}
# {0.0: 30215,
#  0.71068366555606532: 1,
#  0.72670476576861209: 1,
#  0.73890979243259802: 1, 
#  ...}
#+end_src

So something is wrong with the data.

The a(*X) call is also confusing to me. I asked again in the
google group.



** feature vs model selection
I tried a few features, with unexpected (poor) and otherwise
insignificant impact on the model performance

Lets consider something, about toying with features. 

#+begin_src python :results output
# https://docs.python.org/2/library/itertools.html#itertools.combinations
import itertools as iter

def count_iterable(i):
    return sum(1 for e in i)

num_vars = [x for x in range(10)]
num_combinations = {}

for vars in num_vars:
    var_list = [var for var in range(vars)]
    num_combinations[vars] = 0
    for subsets in [y for y in var_list]:
        combinations = iter.combinations(var_list, subsets)
        num_combinations[vars] += count_iterable(combinations)

print num_combinations
#+end_src

#+RESULTS:
: {0: 0, 1: 1, 2: 3, 3: 7, 4: 15, 5: 31, 6: 63, 7: 127, 8: 255, 9: 511}

There we have it, even 4 or 5 features, playing with them
manually becomes unruly.

With regression, we have some feature selection estimators,
i.e. lasso.

What about classification... decision trees should only use
the optimal split. But they'll use some split even if
there's not a good one.

So, is it more effective to begin already with the model
search? Or do we have to search the feature model space?

This already highlights the difficulty with lacking domain
knowledge. For the driver telematics I have a much better
chance and specifying a pretty full feature space, but for
plankton ID and image processing, where I know so little...
sheesh.

So, From here, given my efforts in toying with features have
been unsuccessful, my focus goes towards trying different
models and their parameter space. Then perhaps some progress
can be made here.
** hu central moments Research
   - copied from above
   - and see [[Hu%20central%20moment%20expirimentation][Hu central moment expirimentation]] for tests to
     try
   - see the wiki [[http://en.wikipedia.org/wiki/Image_moment][here]]
   - image moments are certain averages of image pixel
     intensities, and functions of such first moments
   - this is defined for discrete greyscale data as

\begin{equation}
  M_{ij} = \sum_x \sum_y x^i y^j I(x,y)
\end{equation}

   - they usually are formulated to have an attractive
     property, e.g. the hu central moments are rotation,
     translation, reflection?

:HIDDEN:
\begin{equation}
   I_1 = \eta_{20} + \eta_{02}
\end{equation}
\begin{equation}
   I_2 = (\eta_{20} - \eta_{02})^2 + 4\eta_{11}^2
\end{equation}
\begin{equatio}
   I_3 = (\eta_{30} - 3\eta_{12})^2 + (3\eta_{21} - \eta_{03})^2
\end{equation}
\begin{equatio}
   I_4 = (\eta_{30} + \eta_{12})^2 + (\eta_{21} + \eta_{03})^2
\end{equation}
\begin{equatio}
   I_5 = (\eta_{30} - 3\eta_{12}) (\eta_{30} + \eta_{12})[ (\eta_{30} + \eta_{12})^2 - 3 (\eta_{21} + \eta_{03})^2] + (3 \eta_{21} - \eta_{03}) (\eta_{21} + \eta_{03})[ 3(\eta_{30} + \eta_{12})^2 -  (\eta_{21} + \eta_{03})^2]
\end{equation}
\begin{equatio}
   I_6 =  (\eta_{20} - \eta_{02})[(\eta_{30} + \eta_{12})^2 - (\eta_{21} + \eta_{03})^2] + 4\eta_{11}(\eta_{30} + \eta_{12})(\eta_{21} + \eta_{03})
\end{equation}
\begin{equatio}
   I_7 = (3 \eta_{21} - \eta_{03})(\eta_{30} + \eta_{12})[(\eta_{30} + \eta_{12})^2 - 3(\eta_{21} + \eta_{03})^2] - (\eta_{30} - 3\eta_{12})(\eta_{21} + \eta_{03})[3(\eta_{30} + \eta_{12})^2 - (\eta_{21} + \eta_{03})^2].
\end{equation}
:END:

   - how does this transfer to image/pattern recognition?
     Any image can be represented as a density function in
     x,y, which can further be represented by these moments.
     these moments should be invariant with respect to the
     images position in the visual field and pattern size.
   - so *why* does the python method for getting these
     moments take a region centroid? maybe it's to mask out
     parts of the image, but in this case, where the rest of
     the image is blank, I don't see it mattering
   - so, of these moments, 7 may be bad as it represents
     reflected images, and 1, the centroid, I don't think is
     pertinent
   - what of the others? the original [[http://www.sci.utah.edu/~gerig/CS7960-S2010/handouts/Hu.pdf][ieee transactions]]
     document
   - regardless, my question is, if i add crappy features,
     why is the classifier using them and scoring so
     poorly?!
** python vs R
   |-------------------+------------------+---------------------------|
   | R                 | python           | notes                     |
   |-------------------+------------------+---------------------------|
   | dynamically typed | statically typed |                           |
   | caret             | scikitlearn      | sklearn better API        |
   | functional        | object oriented  | in the sense that methods |
   |                   |                  | belong to functions in R  |
   |                   |                  |                           |

* TODO pylearn 2 convolutional neural networks
** DONE setup pylearn 2 dependencies [2/3]
   CLOSED: [2015-02-05 Thu 12:51]
   :LOGBOOK:  
   - State "DONE"       from "TODO"       [2015-02-05 Thu 12:51]
   :END:      
   - [[http://deeplearning.net/software/pylearn2/#dependencies][pylearn 2 deps docs]]

*** DONE theano, might want bleeding edge
    CLOSED: [2015-01-11 Sun 12:58]
    :LOGBOOK:  
    - State "DONE"       from "NEXT"       [2015-01-11 Sun 12:58]
    - State "NEXT"       from "DONE"       [2015-01-11 Sun 12:58]
    :END:      
     - [[http://deeplearning.net/software/theano/install.html#bleeding-edge-install-instructions][install with pip]]
     - [X] run tests
#+begin_src python :noeval
import theano
theano.test()
#+end_src
     - I'm getting issues with the nosetest. S and K instead
       of '.'. What are these? Can't find in docs.
     - S is skip.
     - K is knownfail.

*** DONE PyYAML
    CLOSED: [2015-01-11 Sun 13:19]
    :LOGBOOK:  
    - State "DONE"       from "NEXT"       [2015-01-11 Sun 13:19]
    - State "NEXT"       from "TODO"       [2015-01-11 Sun 13:00]
    :END:      
#+begin_src sh :noeval
pip install PyYAML
#+end_src

*** NEXT PIL for some image related functionality
    :LOGBOOK:  
    - State "NEXT"       from "TODO"       [2015-01-11 Sun 13:20]
    :END:      
    - 
** DONE install pylearn2
   CLOSED: [2015-01-11 Sun 13:40]
   :LOGBOOK:  
   - State "DONE"       from "NEXT"       [2015-01-11 Sun 13:40]
   - State "NEXT"       from ""           [2015-01-11 Sun 13:36]
   :END:      
   - clone from github
   - in venv 
#+begin_src python:noeval
python setup.py develop
#+end_src

** DONE [[https://github.com/zygmuntz/pylearn2-practice][first tutorial pylearn2 in practice]]
   CLOSED: [2015-01-11 Sun 13:58]
   :LOGBOOK:  
   - State "DONE"       from "NEXT"       [2015-01-11 Sun 13:58]
   - State "NEXT"       from ""           [2015-01-11 Sun 13:03]
   :END:      
   - https://github.com/zygmuntz/pylearn2-practice
   - it ran. that was just to verify it works!
** DONE setup environment vars
   CLOSED: [2015-01-11 Sun 14:16]
   :LOGBOOK:  
   - State "DONE"       from "NEXT"       [2015-01-11 Sun 14:16]
   - State "NEXT"       from ""           [2015-01-11 Sun 13:00]
   :END:      
   These are used for some tutorials and test.

#+begin_src sh :noeval
export PYLEARN2_DATA_PATH=/data/lisa/data
export PATH=$PATH:???/pylearn2/scripts
#+end_src

** NEXT [[http://nbviewer.ipython.org/github/lisa-lab/pylearn2/blob/master/pylearn2/scripts/tutorials/convolutional_network/convolutional_network.ipynb][mnist tutorial]]
   :LOGBOOK:  
   - State "NEXT"       from ""           [2015-01-11 Sun 13:00]
   :END:      
   - ok, we know that pylearn2 is working. it'd be good to
     go through the minst tutorial and get to grips with the
     more complex yaml/data.
   - get mnist

** NEXT Build new requirements.txt
   :LOGBOOK:  
   - State "NEXT"       from "TODO"       [2015-01-11 Sun 14:15]
   :END:      
   - 

** TODO setup pylearn2 for our data [1/5]
*** DONE Overview of setting up expiriment 
    CLOSED: [2015-01-11 Sun 14:42]
    :LOGBOOK:  
    - State "DONE"       from "NEXT"       [2015-01-11 Sun 14:42]
    - State "NEXT"       from ""           [2015-01-11 Sun 14:28]
    :END:      
 [[http://deeplearning.net/software/pylearn2/yaml_tutorial/index.html#yaml-tutorial][guide]]

Pylearn2 expiriments are python objects of the type
pylearn2.train.Train. 

- a *dataset*, of type pylearn2.datasets.dataset.Dataset
- a *model*, of type pylearn2.models.model.Model
- a *training algorithm*, of type
  pylearn2.training_algorithms.training_algorithm.TrainingAlgorithm

They are setup via a yaml configuration file such as this:

:HIDDEN:
```
!obj:pylearn2.train.Train {
    "dataset": !obj:pylearn2.datasets.dense_design_matrix.DenseDesignMatrix &dataset {
        "X" : !obj:numpy.random.normal { 'size':[5,3] },
    },
    "model": !obj:pylearn2.models.autoencoder.DenoisingAutoencoder {
        "nvis" : 3,
        "nhid" : 4,
        "irange" : 0.05,
        "corruptor": !obj:pylearn2.corruption.BinomialCorruptor {
            "corruption_level": 0.5,
        },
        "act_enc": "tanh",
        "act_dec": null,    # Linear activation on the decoder side.
    },
    "algorithm": !obj:pylearn2.training_algorithms.sgd.SGD {
        "learning_rate" : 1e-3,
        "batch_size" : 5,
        "monitoring_dataset" : *dataset,
        "cost" : !obj:pylearn2.costs.autoencoder.MeanSquaredReconstructionError {},
        "termination_criterion" : !obj:pylearn2.termination_criteria.EpochCounter {
            "max_epochs": 1,
        },
    },
    "save_path": "./garbage.pkl"
}
```
:END:

It's clear we need to setup each of these components.

*** NEXT vincent domoulin tutorial
    :LOGBOOK:  
    - State "NEXT"       from ""           [2015-01-11 Sun 16:34]
    :END:      
    - http://vdumoulin.github.io/articles/extending-pylearn2/
*** NEXT Dataset
    :LOGBOOK:  
    - State "NEXT"       from ""           [2015-01-11 Sun 14:41]
    :END:      
The data needs to be setup. Pylearn expects it as a pylearn
data object. Useful source for thes is at
- pylearn2/pylearn2/datasets/dense_design_matrix.py
- pylearn2/pylearn2/datasets/dataset.py
- 

The first thing to note is what is required of the dataset
object.

**** NEXT vector representation
     :LOGBOOK:  
     - State "NEXT"       from ""           [2015-01-11 Sun 15:48]
     :END:      
With the DenseDesignMatrix class we can just use a 2d array
for images, provided they all have the same length. Lets
build such a thing.

Remember, the yaml is specified for the dataset object as
follows.

```
dataset: &train !obj:pylearn2.datasets.mnist.MNIST {
        which_set: 'train',
        start: 0,
        stop: 50000
    },
```

So our work should have a config file with something like
the following

```
dataset: &train !obj:pylearn2.datasets.plankton.PLANKTON
```

the which_set and other things removed from that yaml congif
are arguments to the MNIST ~__init__~ method.

Lets have a look at that class. I need to spend the time
necessary to understand what's going on with this and
prepare the plankton data in the same way.

#+begin_src sh :results verbatim org replace output 
cat ~/projects/2014-12-20_datascibowl/pylearn2/pylearn2/datasets/mnist.py
#+end_src

#+RESULTS:
#+BEGIN_SRC python
"""
The MNIST dataset.
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import numpy as N
np = N
from theano.compat.six.moves import xrange
from pylearn2.datasets import dense_design_matrix
from pylearn2.datasets import control
from pylearn2.datasets import cache
from pylearn2.utils import serial
from pylearn2.utils.mnist_ubyte import read_mnist_images
from pylearn2.utils.mnist_ubyte import read_mnist_labels
from pylearn2.utils.rng import make_np_rng


class MNIST(dense_design_matrix.DenseDesignMatrix):
    """
    The MNIST dataset

    Parameters
    ----------
    which_set : str
        'train' or 'test'
    center : bool
        If True, preprocess so that each pixel has zero mean.
    shuffle : WRITEME
    binarize : WRITEME
    start : WRITEME
    stop : WRITEME
    axes : WRITEME
    preprocessor : WRITEME
    fit_preprocessor : WRITEME
    fit_test_preprocessor : WRITEME
    """

    def __init__(self, which_set, center=False, shuffle=False,
                 binarize=False, start=None, stop=None,
                 axes=['b', 0, 1, 'c'],
                 preprocessor=None,
                 fit_preprocessor=False,
                 fit_test_preprocessor=False):
        self.args = locals()

        if which_set not in ['train', 'test']:
            if which_set == 'valid':
                raise ValueError(
                    "There is no such thing as the MNIST validation set. MNIST"
                    "consists of 60,000 train examples and 10,000 test"
                    "examples. If you wish to use a validation set you should"
                    "divide the train set yourself. The pylearn2 dataset"
                    "implements and will only ever implement the standard"
                    "train / test split used in the literature.")
            raise ValueError(
                'Unrecognized which_set value "%s".' % (which_set,) +
                '". Valid values are ["train","test"].')

        def dimshuffle(b01c):
            """
            .. todo::

                WRITEME
            """
            default = ('b', 0, 1, 'c')
            return b01c.transpose(*[default.index(axis) for axis in axes])

        if control.get_load_data():
            path = "${PYLEARN2_DATA_PATH}/mnist/"
            if which_set == 'train':
                im_path = path + 'train-images-idx3-ubyte'
                label_path = path + 'train-labels-idx1-ubyte'
            else:
                assert which_set == 'test'
                im_path = path + 't10k-images-idx3-ubyte'
                label_path = path + 't10k-labels-idx1-ubyte'
            # Path substitution done here in order to make the lower-level
            # mnist_ubyte.py as stand-alone as possible (for reuse in, e.g.,
            # the Deep Learning Tutorials, or in another package).
            im_path = serial.preprocess(im_path)
            label_path = serial.preprocess(label_path)

            # Locally cache the files before reading them
            datasetCache = cache.datasetCache
            im_path = datasetCache.cache_file(im_path)
            label_path = datasetCache.cache_file(label_path)

            topo_view = read_mnist_images(im_path, dtype='float32')
            y = np.atleast_2d(read_mnist_labels(label_path)).T
        else:
            if which_set == 'train':
                size = 60000
            elif which_set == 'test':
                size = 10000
            else:
                raise ValueError(
                    'Unrecognized which_set value "%s".' % (which_set,) +
                    '". Valid values are ["train","test"].')
            topo_view = np.random.rand(size, 28, 28)
            y = np.random.randint(0, 10, (size, 1))

        if binarize:
            topo_view = (topo_view > 0.5).astype('float32')

        y_labels = 10

        m, r, c = topo_view.shape
        assert r == 28
        assert c == 28
        topo_view = topo_view.reshape(m, r, c, 1)

        if which_set == 'train':
            assert m == 60000
        elif which_set == 'test':
            assert m == 10000
        else:
            assert False

        if center:
            topo_view -= topo_view.mean(axis=0)

        if shuffle:
            self.shuffle_rng = make_np_rng(
                None, [1, 2, 3], which_method="shuffle")
            for i in xrange(topo_view.shape[0]):
                j = self.shuffle_rng.randint(m)
                # Copy ensures that memory is not aliased.
                tmp = topo_view[i, :, :, :].copy()
                topo_view[i, :, :, :] = topo_view[j, :, :, :]
                topo_view[j, :, :, :] = tmp

                tmp = y[i:i + 1].copy()
                y[i] = y[j]
                y[j] = tmp

        super(MNIST, self).__init__(topo_view=dimshuffle(topo_view), y=y,
                                    axes=axes, y_labels=y_labels)

        assert not N.any(N.isnan(self.X))

        if start is not None:
            assert start >= 0
            if stop > self.X.shape[0]:
                raise ValueError('stop=' + str(stop) + '>' +
                                 'm=' + str(self.X.shape[0]))
            assert stop > start
            self.X = self.X[start:stop, :]
            if self.X.shape[0] != stop - start:
                raise ValueError("X.shape[0]: %d. start: %d stop: %d"
                                 % (self.X.shape[0], start, stop))
            if len(self.y.shape) > 1:
                self.y = self.y[start:stop, :]
            else:
                self.y = self.y[start:stop]
            assert self.y.shape[0] == stop - start

        if which_set == 'test':
            assert fit_test_preprocessor is None or \
                (fit_preprocessor == fit_test_preprocessor)

        if self.X is not None and preprocessor:
            preprocessor.apply(self, fit_preprocessor)

    def adjust_for_viewer(self, X):
        """
        .. todo::

            WRITEME
        """
        return N.clip(X * 2. - 1., -1., 1.)

    def adjust_to_be_viewed_with(self, X, other, per_example=False):
        """
        .. todo::

            WRITEME
        """
        return self.adjust_for_viewer(X)

    def get_test_set(self):
        """
        .. todo::

            WRITEME
        """
        args = {}
        args.update(self.args)
        del args['self']
        args['which_set'] = 'test'
        args['start'] = None
        args['stop'] = None
        args['fit_preprocessor'] = args['fit_test_preprocessor']
        args['fit_test_preprocessor'] = None
        return MNIST(**args)


class MNIST_rotated_background(dense_design_matrix.DenseDesignMatrix):

    """
    .. todo::

        WRITEME

    Parameters
    ----------
    which_set : WRITEME
    center : WRITEME
    """

    def __init__(self, which_set, center=False):
        path = "${PYLEARN2_DATA_PATH}/mnist/mnist_rotation_back_image/" \
            + which_set

        obj = serial.load(path)
        X = obj['data']
        X = N.cast['float32'](X)
        y = N.asarray(obj['labels'])

        if center:
            X -= X.mean(axis=0)

        view_converter = dense_design_matrix.DefaultViewConverter((28, 28, 1))

        super(MNIST_rotated_background, self).__init__(
            X=X, y=y, y_labels=10, view_converter=view_converter)

        assert not N.any(N.isnan(self.X))
#+END_SRC

**** NEXT conv2d representation
     :LOGBOOK:  
     - State "NEXT"       from ""           [2015-01-11 Sun 15:49]
     :END:      
We can also do images that are not of the same dimensions.
First can try converting the np array of DenseDesignMatrix
into a conv2d representation.
https://groups.google.com/forum/#!topic/pylearn-users/IMxKyzrDKWM

Then we can try setting up the conv2d representation on the
raw images manually.

I'm not 

*** NEXT Model
    :LOGBOOK:  
    - State "NEXT"       from ""           [2015-01-11 Sun 14:41]
    :END:      
    - 

*** NEXT training algorithm
    :LOGBOOK:  
    - State "NEXT"       from ""           [2015-01-11 Sun 14:41]
    :END:      
    - 


